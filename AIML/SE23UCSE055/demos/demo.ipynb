{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892d13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "base_dir = Path(os.getcwd()).resolve().parent\n",
    "sys.path.append(str(base_dir))\n",
    "\n",
    "import src\n",
    "from src import nn\n",
    "from src import optim\n",
    "from src.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7ceff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (60000, 785) | test: (10000, 785)\n",
      "normalized data-splits\n",
      "train_dataset: 60000 | test_dataset: 10000\n",
      "Get random samples | X: (1, 784) | y: tensor([1], requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and prep it (kinda)\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "\n",
    "# Load from dir\n",
    "train_data = np.loadtxt(\"./data/train.csv\", delimiter=\",\", skiprows=1)  # skip header\n",
    "test_data = np.loadtxt(\"./data/test.csv\", delimiter=\",\", skiprows=1)  # skip header\n",
    "print(f\"train: {train_data.shape} | test: {test_data.shape}\")\n",
    "\n",
    "# Normalize\n",
    "X_train = train_data[:, 1:] / 255.\n",
    "y_train = train_data[:, 0].astype(int)\n",
    "X_test = test_data[:, 1:] / 255.\n",
    "y_test = test_data[:, 0].astype(int)\n",
    "print(f\"normalized data-splits\")\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, split: Literal[\"train\", \"test\"], batch_size: int = 1):\n",
    "        self.X_split = X_train if split == \"train\" else X_test\n",
    "        self.y_split = y_train if split == \"train\" else y_test\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y_split) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        start = idx * self.batch_size\n",
    "        end = min((idx + 1) * self.batch_size, len(self.y_split))\n",
    "\n",
    "        batch_X = self.X_split[start:end]\n",
    "        batch_y = self.y_split[start:end]\n",
    "\n",
    "        return src.Tensor(batch_X), src.Tensor(batch_y)  # Targets can stay as np.ndarray for now\n",
    "    \n",
    "train_dataset = Dataset(split=\"train\")\n",
    "test_dataset = Dataset(split=\"test\")\n",
    "print(f\"train_dataset: {len(train_dataset)} | test_dataset: {len(test_dataset)}\")\n",
    "\n",
    "X, y = train_dataset[np.random.randint(low=0, high=len(train_dataset))]\n",
    "print(f\"Get random samples | X: {X.shape} | y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767cf72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train_loss: 0.7452 | train_acc: 80.43% | test_loss: 0.3445 | test_acc: 90.53%\n",
      "Epoch 2/5 | train_loss: 0.3181 | train_acc: 90.90% | test_loss: 0.2697 | test_acc: 92.64%\n",
      "Epoch 3/5 | train_loss: 0.2658 | train_acc: 92.44% | test_loss: 0.2334 | test_acc: 93.49%\n",
      "Epoch 4/5 | train_loss: 0.2325 | train_acc: 93.37% | test_loss: 0.2063 | test_acc: 94.43%\n",
      "Epoch 5/5 | train_loss: 0.2063 | train_acc: 94.21% | test_loss: 0.1844 | test_acc: 94.93%\n"
     ]
    }
   ],
   "source": [
    "# training a model\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "train_dataset = Dataset(split=\"train\", batch_size=batch_size)\n",
    "test_dataset = Dataset(split=\"test\", batch_size=batch_size)\n",
    "\n",
    "\n",
    "def accuracy_fn(logits, targets):\n",
    "    if isinstance(targets, src.Tensor):\n",
    "        targets = targets.data\n",
    "    preds = np.argmax(logits.data, axis=-1)\n",
    "    return (preds == targets).astype(np.float32).mean()\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10, bias=True),\n",
    ")\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=1e-2)\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train-step\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    indices = np.random.permutation(len(train_dataset))\n",
    "    for i in indices:\n",
    "        X, y = train_dataset[i]\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy_loss(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data\n",
    "        train_acc += accuracy_fn(logits, y)\n",
    "    \n",
    "    # test-step\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    indices = np.random.permutation(len(test_dataset))\n",
    "    for i in indices:\n",
    "        X, y = train_dataset[i]\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy_loss(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        test_loss += loss.data\n",
    "        test_acc += accuracy_fn(logits, y)\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_acc /= len(test_dataset)\n",
    "\n",
    "    results[\"train_loss\"].append(float(train_loss))\n",
    "    results[\"train_acc\"].append(float(train_acc))\n",
    "    results[\"test_loss\"].append(float(test_loss))\n",
    "    results[\"test_acc\"].append(float(test_acc))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | train_loss: {train_loss:.4f} | train_acc: {train_acc*100:.2f}% | test_loss: {test_loss:.4f} | test_acc: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4147c98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.7452069667786362,\n",
       "  0.31814326080513516,\n",
       "  0.2658484651418133,\n",
       "  0.23252920236368826,\n",
       "  0.20632705522404887],\n",
       " 'train_acc': [0.8043166399002075,\n",
       "  0.9090499877929688,\n",
       "  0.9243500232696533,\n",
       "  0.9337000250816345,\n",
       "  0.9421333074569702],\n",
       " 'test_loss': [0.34448568712669314,\n",
       "  0.26969813878526966,\n",
       "  0.23342200062447518,\n",
       "  0.20630891293219572,\n",
       "  0.18439942301336493],\n",
       " 'test_acc': [0.9052516222000122,\n",
       "  0.9264177083969116,\n",
       "  0.934904158115387,\n",
       "  0.9442891478538513,\n",
       "  0.9492811560630798]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6e066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (basic)",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
